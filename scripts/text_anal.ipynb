{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    import re\n",
    "    \" removes urls\"\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    links_lst = re.findall(url_pattern, text)\n",
    "    return re.sub(url_pattern, \"\", text), links_lst # URL_MASK\n",
    "    \n",
    "def remove_parentheses(text):\n",
    "    import re\n",
    "    \" removes parentheses\"\n",
    "    para_pattern = re.compile(r'\\(.*?\\)')\n",
    "    para_lst = [re.sub(\"[\\\\(\\\\)]\", \"\", x).strip() for x in re.findall(para_pattern, text)]\n",
    "    return re.sub(para_pattern, \"\", text), para_lst # PARENTHESES_MASK\n",
    "    \n",
    "def remove_html(text):\n",
    "    import re\n",
    "    \" removes html tags\"\n",
    "    html_pattern = re.compile('')\n",
    "    return html_pattern.sub(r'', text)\n",
    "\n",
    "def remove_new_line(text):\n",
    "    import re\n",
    "    return re.sub(r\"\\\\\\n\", ' ', text)\n",
    "\n",
    "def remove_non_alpha(text):\n",
    "    import re\n",
    "    return re.sub(\"[^A-Za-z0-9\\\\.\\\\_]+\", ' ', str(text))\n",
    "\n",
    "def fix_these_data(text, fix_lst):\n",
    "    import re\n",
    "    if text[0] != \" \":\n",
    "        text = \" \" + text\n",
    "    if text[-1] != \" \":\n",
    "        text = text + \" \"\n",
    "    \n",
    "    for x in fix_lst:\n",
    "        text = re.sub(x[0], x[1], text)\n",
    "    text = re.sub(\"\\\\(\", \" ( \", text)\n",
    "    text = re.sub(\"\\\\)\", \" ) \", text)\n",
    "    text = re.sub(\"\\\\.\\\\\\n\", \" \", text)\n",
    "    text = re.sub(\"\\\\\\\"\", \" \", text)\n",
    "    text = re.sub(\"\\\\\\'\", \" \", text)\n",
    "    text = re.sub(\" +\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def remove_extra_periods(text):\n",
    "    import re\n",
    "    pat = re.compile('\\\\.')\n",
    "    txt_lst = re.sub(pat, \" NJ3CT_H3R3 \", text).split(\"NJ3CT_H3R3\")\n",
    "    hodl_string = \" \".join(txt_lst)\n",
    "    \n",
    "    if len(txt_lst) > 1:\n",
    "        temp_lst = []\n",
    "        hodl_string = \"\"\n",
    "        for txt_part in txt_lst:\n",
    "            txt_part = txt_part.strip()\n",
    "            if len(txt_part) > 0:\n",
    "                if txt_part[0] == txt_part[0].lower():\n",
    "                    hodl_string = hodl_string + \" \" + txt_part\n",
    "                else:\n",
    "                    hodl_string = hodl_string + \". \" + txt_part\n",
    "                    if hodl_string[0] == \".\":\n",
    "                        hodl_string = hodl_string[1:]\n",
    "    return hodl_string.strip()\n",
    "\n",
    "def preprocess_text(t): #[\" UK \", \" United Kingdom \"], [\" U\\\\.K\\\\. \", \" United Kingdom \"], [\" U\\\\. K\\\\. \", \" United Kingdom \"], [\" U K \", \" United Kingdom \"], [\" U\\\\. S\\\\. \", \" United States \"], [\" US \", \" United States \"], [\" U S \", \" United States \"], [\" U\\\\.S\\\\. \", \" United States \"], \n",
    "    import re\n",
    "    # contraction_lst = [[\" can\\\\\\'t \",  \" can not \"], [\" let\\\\\\'s \",  \" let us \"], [\" won\\\\\\'t \",  \" will not \"], [\" n\\\\\\'t \",  \"  not \"], [\" \\\\\\'m \",  \"  am \"], [\" \\\\\\'s \",  \"  is \"], [\" \\\\\\'re \",  \"  are \"], [\" \\\\\\'ve \",  \"  have \"], [\" \\\\\\'d \",  \"  had \"], [\" \\\\\\'ll \",  \"  will \"], [\" arent \",  \" are not \"], [\" cant \",  \" cannot \"], [\" couldnt \",  \" could not \"], [\" didnt \",  \" did not \"], [\" doesnt \",  \" does not \"], [\" dont \",  \" do not \"], [\" hadnt \",  \" had not \"], [\" hasnt \",  \" has not \"], [\" havent \",  \" have not \"], [\" hed \",  \" he had \"], [\" hes \",  \" he is \"], [\" Id \",  \" I would \"], [\" Ill \",  \" I will \"], [\" Im \",  \" I am \"], [\" Ive \",  \" I have \"], [\" isnt \",  \" is not \"], [\" lets \",  \" let us \"], [\" mightnt \",  \" might not \"], [\" mustnt \",  \" must not \"], [\" shant \",  \" shall not \"], [\" shed \",  \" she had \"], [\" shes \",  \" she is \"], [\" shouldnt \",  \" should not \"], [\" thats \",  \" that is \"], [\" theres \",  \" there is \"], [\" theyd \",  \" they had \"], [\" theyll \",  \" they will \"], [\" theyre \",  \" they are \"], [\" theyve \",  \" they have \"], [\" wed \",  \" we had \"], [\" were \",  \" we are \"], [\" weve \",  \" we have \"], [\" werent \",  \" were not \"], [\" whatll \",  \" what will \"], [\" whatre \",  \" what are \"], [\" whats \",  \" what is \"], [\" whatve \",  \" what have \"], [\" wheres \",  \" where is \"], [\" whod \",  \" who had \"], [\" wholl \",  \" who will \"], [\" whore \",  \" who are \"], [\" whos \",  \" who is \"], [\" whove \",  \" who have \"], [\" wont \",  \" will not \"], [\" wouldnt \",  \" would not \"], [\" youd \",  \" you had \"], [\" youll \",  \" you will \"], [\" youre \",  \" you are \"], [\" youve \",  \" you have \"]]\n",
    "    contraction_lst = [[\" quot \", \"\"], [\" cant \",  \" can not \"], [\" lets \",  \" let us \"], [\" wont \",  \" will not \"], [\" nt \",  \"  not \"], [\" m \",  \"  am \"], [\" s \",  \"  is \"], [\" re \",  \"  are \"], [\" ve \",  \"  have \"], [\" d \",  \"  had \"], [\" ll \",  \"  will \"], [\" arent \",  \" are not \"], [\" cant \",  \" cannot \"], [\" couldnt \",  \" could not \"], [\" didnt \",  \" did not \"], [\" doesnt \",  \" does not \"], [\" dont \",  \" do not \"], [\" hadnt \",  \" had not \"], [\" hasnt \",  \" has not \"], [\" havent \",  \" have not \"], [\" hed \",  \" he had \"], [\" hes \",  \" he is \"], [\" Id \",  \" I would \"], [\" Ill \",  \" I will \"], [\" Im \",  \" I am \"], [\" Ive \",  \" I have \"], [\" isnt \",  \" is not \"], [\" lets \",  \" let us \"], [\" mightnt \",  \" might not \"], [\" mustnt \",  \" must not \"], [\" shant \",  \" shall not \"], [\" shed \",  \" she had \"], [\" shes \",  \" she is \"], [\" shouldnt \",  \" should not \"], [\" thats \",  \" that is \"], [\" theres \",  \" there is \"], [\" theyd \",  \" they had \"], [\" theyll \",  \" they will \"], [\" theyre \",  \" they are \"], [\" theyve \",  \" they have \"], [\" wed \",  \" we had \"], [\" were \",  \" we are \"], [\" weve \",  \" we have \"], [\" werent \",  \" were not \"], [\" whatll \",  \" what will \"], [\" whatre \",  \" what are \"], [\" whats \",  \" what is \"], [\" whatve \",  \" what have \"], [\" wheres \",  \" where is \"], [\" whod \",  \" who had \"], [\" wholl \",  \" who will \"], [\" whore \",  \" who are \"], [\" whos \",  \" who is \"], [\" whove \",  \" who have \"], [\" wont \",  \" will not \"], [\" wouldnt \",  \" would not \"], [\" youd \",  \" you had \"], [\" youll \",  \" you will \"], [\" youre \",  \" you are \"], [\" youve \",  \" you have \"]]\n",
    "    t = re.sub(r\"'\", \"\", t)\n",
    "    t = re.sub(r\",\", \"\", t)\n",
    "    t = fix_these_data(t, contraction_lst)\n",
    "    t, link_lst = remove_urls(t)\n",
    "    t, para_lst = remove_parentheses(t)\n",
    "    t = remove_extra_periods(t)\n",
    "    t = remove_html(t)\n",
    "    t = remove_new_line(t)\n",
    "    t = remove_non_alpha(t)\n",
    "    t = lemmatize_words(t)\n",
    "    t = remove_stopwords(t)\n",
    "    return t.lower().replace(\"\\\\.\", \"\").strip()\n",
    "\n",
    "#################################################################\n",
    "\n",
    "def sent_anal(text):\n",
    "    sent_res = classifier(text)\n",
    "    return [sent_res[0][\"label\"], sent_res[0][\"score\"]]\n",
    "\n",
    "def get_parts_of_speech(doc):\n",
    "    return [[token.text, token.pos_] for token in doc]\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in stopwords])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import statistics, spacy, time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords = stopwords.words(\"english\")\n",
    "classifier = pipeline(\"sentiment-analysis\", model = \"siebert/sentiment-roberta-large-english\")\n",
    "\n",
    "data_pth = \"N:/Classes/2022_2FALL/Analytics Day/Data/output_df.csv\"\n",
    "\n",
    "df = pd.read_csv(data_pth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEXT ANALYSIS PACKAGES INIT AND HYPERPARAMETER LIST\n",
    "\n",
    "##################################################################################################################################################\n",
    "\n",
    "from bertopic import BERTopic\n",
    "import os, spacy\n",
    "from umap import UMAP\n",
    "from sklearn.decomposition import PCA\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from transformers.pipelines import pipeline\n",
    "import nltk, time, umap, hdbscan, gc\n",
    "from nltk.corpus import stopwords\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "verbose = True\n",
    "rand_state = 20221207\n",
    "\n",
    "\n",
    "nn_list = [10, 25, 50, 75, 90]\n",
    "nc_list = [2]\n",
    "lr_list = [0.01]\n",
    "md_list = [0.05]\n",
    "m_list = [\"cosine\"] #  \n",
    "\n",
    "min_cluster_size_list = [10]\n",
    "\n",
    "ng_list = [(1, 1)] # [(1, 1), (1, 2), (1, 3)]\n",
    "tnw_list = [10]\n",
    "mts_list = [10]\n",
    "nrt_list = [None] # , 10, \"auto\"\n",
    "\n",
    "temp_fold_pth = \"N:/Classes/2022_2FALL/Analytics/Day/topic_model/\"\n",
    "\n",
    "##################################################################################################################################################################\n",
    "##################################################################################################################################################################\n",
    "##################################################################################################################################################################\n",
    "\n",
    "bertopic_list = []\n",
    "docs = df[\"clean_rComments_tags\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BERTopic with UMAP, HDBSCAN, COUNTVECTORIZER, and CTFIDF models\n",
    "##################################################################################################################################################\n",
    "\n",
    "embedding_model_bert = pipeline(\n",
    "                                \"feature-extraction\",\n",
    "                                model = \"sentence-transformers/all-mpnet-base-v2\" #\"distilbert-base-cased\"\n",
    "                               )\n",
    "\n",
    "print(\"NUMBER OF UMAP MODELS CREATED:\", len(nn_list)*len(nc_list)*len(lr_list)*len(md_list)*len(m_list))\n",
    "umap_model_list = []\n",
    "for nn in nn_list:\n",
    "    for nc in nc_list:\n",
    "        for lr in lr_list:\n",
    "            for md in md_list:\n",
    "                for m in m_list:\n",
    "                    umap_model = UMAP(\n",
    "                                      n_neighbors = nn,             # int 2-100, doc neighborhood\n",
    "                                      n_components = nc,            # int 2-100, doc dimensionality\n",
    "                                      n_epochs = 500,              # None 'or' 200 (large dataframes) 'or' 500 (small dataframes)\n",
    "                                      learning_rate = lr,          # float\n",
    "                                    #   init = \"spectral\",            # string spectral, random, pca 'or' np.array\n",
    "                                      min_dist = md,               # float 0.1 (default)\n",
    "                                      metric = m,             # string euclidean, manhattan, chebyshev, minkowski, canberra, braycurtis, mahalanobis, wminkowski, seuclidean, cosine, correlation, haversine, hamming, jaccard, dice, russelrao, kulsinski, ll_dirichlet, hellinger, rogerstanimoto, sokalmichener, sokalsneath, yule\n",
    "                                      verbose = verbose,\n",
    "                                      random_state = rand_state\n",
    "                                     )\n",
    "                    umap_model_list.append([umap_model, nn, nc, lr, md, m])\n",
    "\n",
    "hdbscan_model_list = []\n",
    "print(\"NUMBER OF HDBSCAN MODELS CREATED:\", len(min_cluster_size_list))\n",
    "for mcsl in min_cluster_size_list:\n",
    "    hdbscan_model = HDBSCAN(\n",
    "                            min_cluster_size = mcsl,                  # The minimum size of clusters; single linkage splits that contain fewer points than this will be considered points \"falling out\" of a cluster rather than a cluster splitting into two new clusters.\n",
    "                            metric = \"euclidean\",\n",
    "                            prediction_data = True,\n",
    "                            gen_min_span_tree = True,               # Whether to generate the minimum spanning tree with regard to mutual reachability distance for later analysis.\n",
    "                            # cluster_selection_method = \"eom\",\n",
    "                            # approx_min_span_tree = True,          # Default = False\n",
    "                            # allow_single_cluster = False,\n",
    "                            # min_samples = None,                     # The number of samples in a neighbourhood for a point to be considered a core point.\n",
    "                            # cluster_selection_epsilon = 0,\n",
    "                            # max_cluster_size = 0,\n",
    "                            # alpha = 1,\n",
    "                            # p = None,\n",
    "                            # algorithm = \"best\",\n",
    "                            # leaf_size = 40,\n",
    "                            # match_reference_implementation = False  # There exist some interpretational differences between this HDBSCAN* implementation and the original authors reference implementation in Java.\n",
    "                            )\n",
    "    hdbscan_model_list.append([hdbscan_model, mcsl])\n",
    "\n",
    "\n",
    "vectorizer_model_list = [[CountVectorizer(ngram_range = ng, stop_words = \"english\"), ng] for ng in ng_list]\n",
    "\n",
    "\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words = True)\n",
    "\n",
    "print(\"NUMBER OF BERTopic MODELS CREATED:\", len(hdbscan_model_list)*len(umap_model_list)*len(vectorizer_model_list)*len(tnw_list)*len(mts_list)*len(nrt_list))\n",
    "\n",
    "i = 0\n",
    "for hdbscan_model in hdbscan_model_list:\n",
    "    for umap_model in umap_model_list:\n",
    "        for vectorizer_model in vectorizer_model_list:\n",
    "            for tnw in tnw_list:\n",
    "                for mts in mts_list:\n",
    "                    for nrt in nrt_list:\n",
    "                        start_time = time.time()\n",
    "                        model_nm = temp_fold_pth + \"temp_user_comment\" + \"_topic_model_\" + str(i)\n",
    "                        topic_model = BERTopic(\n",
    "                                            verbose = verbose,\n",
    "                                            language = \"multilingual\",\n",
    "                                            top_n_words = tnw,\n",
    "                                            min_topic_size = mts,\n",
    "                                            nr_topics = nrt,\n",
    "                                            low_memory = False,\n",
    "                                            calculate_probabilities = True,\n",
    "                                            embedding_model = embedding_model_bert,\n",
    "                                            umap_model = umap_model[0],\n",
    "                                            hdbscan_model = hdbscan_model[0],\n",
    "                                            vectorizer_model = vectorizer_model[0],\n",
    "                                            ctfidf_model = ctfidf_model,\n",
    "                                            )\n",
    "\n",
    "                        # Fit the model and predict documents\n",
    "                        topics, probs = topic_model.fit_transform(docs)\n",
    "                        end_time = time.time()\n",
    "                        num_tops = len(topic_model.get_topic_info().Topic.tolist())\n",
    "\n",
    "                        print(\"NUMBER OF TOPICS IN\" + model_nm + \" =\", num_tops)\n",
    "                        # topic_model.save(model_nm)\n",
    "\n",
    "                        bertopic_list.append([i, \"USER COMMENTS\", num_tops, model_nm, start_time, end_time] + umap_model[1:] + hdbscan_model[1:] + vectorizer_model[1:])\n",
    "\n",
    "                        i = i + 1\n",
    "                        del topic_model\n",
    "                        gc.collect()\n",
    "\n",
    "bertopic_df = pd.DataFrame(bertopic_list, columns = [\"index\", \"col_name\", \"num_tops\", \"model_nm\", \"start_time\", \"end_time\", \"near_neighbor\", \"num_components\", \"learn_rate\", \"min_dist\", \"metric\", \"min_clust_size\", \"n_gram\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_list = []\n",
    "print(\"STARTED PREPROCESSING TEXT\")\n",
    "\n",
    "s = time.time()\n",
    "df[\"clean_rComments\"] = df[\"rComments\"].apply(preprocess_text)\n",
    "print(\"CHECKPOINT: clean_rComments\", s)\n",
    "s_list.append([\"clean_rComments\", s])\n",
    "\n",
    "s = time.time()\n",
    "df[\"clean_rComments_tags\"] = df[\"rComments_tags\"].apply(preprocess_text)\n",
    "print(\"CHECKPOINT: clean_rComments_tags\", s)\n",
    "s_list.append([\"clean_rComments_tags\", s])\n",
    "\n",
    "df = df[df[\"clean_rComments\"] != \"\"]\n",
    "\n",
    "print(\"COMPLETED PREPROCESS :: STARTED REMOVING STOPWORDS\") # clean_rComments, clean_rComments_tags\n",
    "\n",
    "\n",
    "s = time.time()\n",
    "df[\"clean_rComments_nostops\"] = df[\"clean_rComments\"].apply(remove_stopwords)\n",
    "print(\"CHECKPOINT: clean_rComments_nostops\", s)\n",
    "s_list.append([\"clean_rComments_nostops\", s])\n",
    "\n",
    "\n",
    "s = time.time()\n",
    "df[\"clean_rComments_tags_nostops\"] = df[\"clean_rComments_tags\"].apply(remove_stopwords)\n",
    "print(\"CHECKPOINT: clean_rComments_tags_nostops\", s)\n",
    "s_list.append([\"clean_rComments_tags_nostops\", s])\n",
    "\n",
    "\n",
    "\n",
    "print(\"COMPLETED REMOVING STOPWORDS :: STARTED LEMMATIZATION\") # clean_rComments, clean_rComments_tags, clean_rComments_nostops, clean_rComments_tags_nostops\n",
    "\n",
    "\n",
    "\n",
    "s = time.time()\n",
    "df[\"clean_rComments_lemma\"] = df[\"clean_rComments\"].apply(lemmatize_words)\n",
    "print(\"CHECKPOINT: clean_rComments_lemma\", s)\n",
    "s_list.append([\"clean_rComments_lemma\", s])\n",
    "\n",
    "\n",
    "s = time.time()\n",
    "df[\"clean_rComments_tags_lemma\"] = df[\"clean_rComments_tags\"].apply(lemmatize_words)\n",
    "print(\"CHECKPOINT: clean_rComments_tags_lemma\", s)\n",
    "s_list.append([\"clean_rComments_tags_lemma\", s])\n",
    "\n",
    "\n",
    "s = time.time()\n",
    "df[\"clean_rComments_nostops_lemma\"] = df[\"clean_rComments_nostops\"].apply(lemmatize_words)\n",
    "print(\"CHECKPOINT: clean_rComments_nostops_lemma\", s)\n",
    "s_list.append([\"clean_rComments_nostops_lemma\", s])\n",
    "\n",
    "\n",
    "s = time.time()\n",
    "df[\"clean_rComments_tags_nostops_lemma\"] = df[\"clean_rComments_tags_nostops\"].apply(lemmatize_words)\n",
    "print(\"CHECKPOINT: clean_rComments_tags_nostops_lemma\", s)\n",
    "s_list.append([\"clean_rComments_tags_nostops_lemma\", s])\n",
    "\n",
    "\n",
    "\n",
    "print(\"COMPLETED LEMMATIZATION\") # clean_rComments, clean_rComments_tags, clean_rComments_nostops, clean_rComments_tags_nostops, clean_rComments_lemma, clean_rComments_tags_lemma, clean_rComments_nostops_lemma, clean_rComments_tags_nostops_lemma\n",
    "\n",
    "\n",
    "# clean_rComments\n",
    "# clean_rComments_tags\n",
    "\n",
    "# clean_rComments_nostops\n",
    "# clean_rComments_tags_nostops\n",
    "\n",
    "# clean_rComments_lemma\n",
    "# clean_rComments_tags_lemma\n",
    "\n",
    "# clean_rComments_nostops_lemma\n",
    "# clean_rComments_tags_nostops_lemma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"clean_rComments_nostops\"] == \"\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STARTED SENTIMENT ANALYSIS\")\n",
    "\n",
    "# clean_rComments\n",
    "s = time.time()\n",
    "print(\"NUMBER OF RECORDS:\", df.loc[df[\"clean_rComments\"] != \"\", \"clean_rComments\"].shape[0])\n",
    "df.loc[df[\"clean_rComments\"] != \"\", \"STUDSENT_rComments\"] = df.loc[df[\"clean_rComments\"] != \"\", \"clean_rComments\"].apply(sent_anal)\n",
    "print(\"CHECKPOINT: STUDSENT_rComments\", s)\n",
    "s_list.append([\"STUDSENT_rComments\", s])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_rComments_tags\n",
    "s = time.time()\n",
    "df.loc[df[\"clean_rComments_tags\"] != \"\", \"STUDSENT_rComments_tags\"] = df.loc[df[\"clean_rComments_tags\"] != \"\", \"clean_rComments_tags\"].apply(sent_anal)\n",
    "print(\"CHECKPOINT: STUDSENT_rComments_tags\", s)\n",
    "s_list.append([\"STUDSENT_rComments_tags\", s])\n",
    "\n",
    "# clean_rComments_lemma\n",
    "s = time.time()\n",
    "df.loc[df[\"clean_rComments_lemma\"] != \"\", \"STUDSENT_rComments_lemma\"] = df.loc[df[\"clean_rComments_lemma\"] != \"\", \"clean_rComments_lemma\"].apply(sent_anal)\n",
    "print(\"CHECKPOINT: STUDSENT_rComments_lemma\", s)\n",
    "s_list.append([\"STUDSENT_rComments_lemma\", s])\n",
    "\n",
    "# clean_rComments_tags_lemma\n",
    "s = time.time()\n",
    "df.loc[df[\"clean_rComments_tags_lemma\"] != \"\", \"STUDSENT_rComments_tags_lemma\"] = df.loc[df[\"clean_rComments_tags_lemma\"] != \"\", \"clean_rComments_tags_lemma\"].apply(sent_anal)\n",
    "print(\"CHECKPOINT: STUDSENT_rComments_tags_lemma\", s)\n",
    "s_list.append([\"STUDSENT_rComments_tags_lemma\", s])\n",
    "\n",
    "# clean_rComments_nostops\n",
    "s = time.time()\n",
    "df.loc[df[\"clean_rComments_nostops\"] != \"\", \"STUDSENT_rComments_nostops\"] = df.loc[df[\"clean_rComments_nostops\"] != \"\", \"clean_rComments_nostops\"].apply(sent_anal)\n",
    "print(\"CHECKPOINT: STUDSENT_rComments_nostops\", s)\n",
    "s_list.append([\"STUDSENT_rComments_nostops\", s])\n",
    "\n",
    "# clean_rComments_tags_nostops\n",
    "s = time.time()\n",
    "df.loc[df[\"clean_rComments_tags_nostops\"] != \"\", \"STUDSENT_rComments_tags_nostops\"] = df.loc[df[\"clean_rComments_tags_nostops\"] != \"\", \"clean_rComments_tags_nostops\"].apply(sent_anal)\n",
    "print(\"CHECKPOINT: STUDSENT_rComments_tags_nostops\", s)\n",
    "s_list.append([\"STUDSENT_rComments_tags_nostops\", s])\n",
    "\n",
    "# clean_rComments_nostops_lemma\n",
    "s = time.time()\n",
    "df[\"STUDSENT_rComments_nostops_lemma\"] = df[\"clean_rComments_nostops_lemma\"].apply(sent_anal)\n",
    "print(\"CHECKPOINT: STUDSENT_rComments_nostops_lemma\", s)\n",
    "s_list.append([\"STUDSENT_rComments_nostops_lemma\", s])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"N:/Classes/2022_2FALL/Analytics Day/Code/output_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "import os, spacy\n",
    "from umap import UMAP\n",
    "from sklearn.decomposition import PCA\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from transformers.pipelines import pipeline\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import umap, hdbscan\n",
    "import gc\n",
    "\n",
    "verbose = True\n",
    "rand_state = 20221207\n",
    "\n",
    "nn_list = [20]\n",
    "nc_list = [2]\n",
    "lr_list = [0.05]\n",
    "md_list = [0.01]\n",
    "m_list = [\"cosine\"]\n",
    "\n",
    "min_cluster_size_list = [10]\n",
    "\n",
    "ng_list = [(1, 3)]\n",
    "\n",
    "tnw_list = [10]\n",
    "mts_list = [10]\n",
    "nrt_list = [None] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "embedding_model_bert = pipeline(\n",
    "                                \"feature-extraction\", \n",
    "                                model = \"sentence-transformers/all-mpnet-base-v2\" #\"distilbert-base-cased\"\n",
    "                               )\n",
    "\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "umap_model_list = []\n",
    "for nn in nn_list:\n",
    "    for nc in nc_list:\n",
    "        for lr in lr_list:\n",
    "            for md in md_list:\n",
    "                for m in m_list:\n",
    "                    umap_model = UMAP(\n",
    "                                      n_neighbors = nn,             # int 2-100, doc neighborhood\n",
    "                                      n_components = nc,            # int 2-100, doc dimensionality\n",
    "                                      n_epochs = 500,              # None 'or' 200 (large dataframes) 'or' 500 (small dataframes)\n",
    "                                      learning_rate = lr,          # float\n",
    "                                    #   init = \"spectral\",            # string spectral, random, pca 'or' np.array \n",
    "                                      min_dist = md,               # float 0.1 (default)\n",
    "                                      metric = m,             # string euclidean, manhattan, chebyshev, minkowski, canberra, braycurtis, mahalanobis, wminkowski, seuclidean, cosine, correlation, haversine, hamming, jaccard, dice, russelrao, kulsinski, ll_dirichlet, hellinger, rogerstanimoto, sokalmichener, sokalsneath, yule\n",
    "                                      verbose = verbose,\n",
    "                                      random_state = rand_state\n",
    "                                     )\n",
    "                    umap_model_list.append([umap_model, nn, nc, lr, md, m])\n",
    "\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "\n",
    "hdbscan_model_list = []\n",
    "for mcsl in min_cluster_size_list:\n",
    "    hdbscan_model = HDBSCAN(\n",
    "                            min_cluster_size = mcsl,                  # The minimum size of clusters; single linkage splits that contain fewer points than this will be considered points \"falling out\" of a cluster rather than a cluster splitting into two new clusters.\n",
    "                            metric = \"euclidean\",\n",
    "                            prediction_data = True,\n",
    "                            gen_min_span_tree = True,               # Whether to generate the minimum spanning tree with regard to mutual reachability distance for later analysis.\n",
    "                            # cluster_selection_method = \"eom\",\n",
    "                            # approx_min_span_tree = True,          # Default = False\n",
    "                            # allow_single_cluster = False,\n",
    "                            # min_samples = None,                     # The number of samples in a neighbourhood for a point to be considered a core point.\n",
    "                            # cluster_selection_epsilon = 0,\n",
    "                            # max_cluster_size = 0,\n",
    "                            # alpha = 1,\n",
    "                            # p = None,\n",
    "                            # algorithm = \"best\",\n",
    "                            # leaf_size = 40,\n",
    "                            # match_reference_implementation = False  # There exist some interpretational differences between this HDBSCAN* implementation and the original authors reference implementation in Java. \n",
    "                            )\n",
    "    hdbscan_model_list.append([hdbscan_model, mcsl])\n",
    "\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "\n",
    "vectorizer_model_list = [[CountVectorizer(ngram_range = ng, stop_words = \"english\"), ng] for ng in ng_list]\n",
    "\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words = True)\n",
    "\n",
    "\n",
    "#######################################################################################################################\n",
    "import time \n",
    "\n",
    "print(\"NUMBER OF TOPIC MODELS: \", len(hdbscan_model_list)*len(umap_model_list)*len(vectorizer_model_list)*len(tnw_list)*len(mts_list)*len(nrt_list))\n",
    "\n",
    "i = 0\n",
    "bertopic_list = []\n",
    "for hdbscan_model in hdbscan_model_list:\n",
    "    for umap_model in umap_model_list: \n",
    "        for vectorizer_model in vectorizer_model_list:\n",
    "            for tnw in tnw_list:\n",
    "                for mts in mts_list:\n",
    "                    for nrt in nrt_list:\n",
    "                        model_nm = \"N:/Classes/2022_2FALL/CARES LAB PROJECT/topic_model_\" + str(i)\n",
    "                        start_time = time.time()\n",
    "                        topic_model = BERTopic(\n",
    "                                            verbose = verbose,\n",
    "                                            language = \"multilingual\",\n",
    "                                            top_n_words = tnw,\n",
    "                                            min_topic_size = mts,\n",
    "                                            nr_topics = nrt,\n",
    "                                            low_memory = False,\n",
    "                                            calculate_probabilities = True,\n",
    "                                            embedding_model = embedding_model_bert,\n",
    "                                            umap_model = umap_model[0],\n",
    "                                            hdbscan_model = hdbscan_model[0],\n",
    "                                            vectorizer_model = vectorizer_model[0],\n",
    "                                            ctfidf_model = ctfidf_model,\n",
    "                                            )\n",
    "\n",
    "                        # Fit the model and predict documents\n",
    "                        topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "                        end_time = time.time()\n",
    "                        time_diff = end_time - start_time\n",
    "\n",
    "                        num_topics = len(topic_model.get_topic_info().Topic.tolist())\n",
    "\n",
    "                        print(\"NUMBER OF TOPICS IN\" + model_nm + \" =\", num_topics)\n",
    "                        topic_model.save(model_nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5210f08b43caa98193ac4cc30e63606a904c7d72d37a54875ef48f7a7b311874"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
