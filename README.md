# Why Withdraw

The goal of this research is to study the pattern of change in the two-year withdrawal rate at post-secondary schools over time while accounting for the average student sentiment to see if a relationship between a school’s average student sentiment and two-year withdrawal rate exists. Data were scraped from Rate My Professors and merged with the CollegeScorecard dataset to determine which post-secondary schools have a positive or negative average student sentiment over time. After which a longitudinal model was used to model the two-year withdrawal rate over time.

To measure student sentiment, a pre-trained language model was used on the comments and tags left by students on Rate My Professors to gauge positive or negative sentiment. The average student sentiment was aggregated across each school for each school year. The two-year withdrawal rate was modeled using the school year and average student sentiment for each school.

Often students polarized by either good or bad teachers navigate to Rate My Professor to voice their opinions and experiences on the courses they took over the previous semester. Utilizing the sentiment left in the comments student reviewers give on these reviews could potentially provide valuable insight into why some schools have high withdrawal rates. The purpose of this project is to use student sentiment to model the pattern of change in the two-year withdrawal rate observed at post-secondary schools over time. 

### __Data Wrangle__    

* __Data Collection:__ The data collection process utilized a GitHub repository under the name ‘ratemyprof-api’ that returns all professor review data for a given school’s SID. Since a school’s SID is assigned by Rate My Professors and is not in the CollegeScorecard dataset, the SID’s must be collected for each school. Python packages ‘Requests’ and ‘BeautifulSoup’ were used to search Rate My Professors for the school’s URL directory and collect 3,988 SID’s. As described under the Data Wrangling section, this project will focus on Georgia schools that offer at least a bachelor’s degree, meaning 97 of 3,988 SID’s were used to collect 348,083 reviews. Some of these reviews were missing comments or contained only non-alphanumeric text and were dropped from the data, which left a total of 303,482 reviews.

* __Data Wrangling:__ Due to the number of reviews Rate My Professors has for each school, this project will focus on post-secondary schools in Georgia where the highest degree offered is either a bachelor’s or a graduate degree. Some schools have very few reviews or only have reviews for one or two school years. Since a longitudinal model will be constructed from this data, these schools were dropped from the dataset. Lastly, the school years prior to 2001-2002 contain little to no data and were not used in this project. After filtering the data in this way 286,288 reviews remained. After aggregating to the school level, a total of 1,122 occasions are observed for 97 Georgia schools over 18 school years (2001-2002 to 2018-2019). 

### __Tools used__     

* SAS  
* Python
* R  

### __Results__    

!["Figures](https://github.com/njones738/Why-Withdraw/blob/main/data/images/exec_sum_figures.JPG)

* __Review Count Per School (Figure 1):__ Figure 1 displays the frequency of reviews over time. The total number of reviews is displayed by the grey bar. The gold bars layered on top are reviews from KSU. This plot shows an increase in review frequency over time with the peak happening the in the 2016-2017 school year. Notably, there was a sizable increase in reviews after the 2008-2009 school year. 
* __Review Sentiment Count (Figure 2):__ Figure 2 displays the results of the sentiment analysis on the review comments, which labelled 78,944 reviews as ‘Negative’ (red) and 207,341 as ‘Positive’ (green). To verify the results, a comparison was made between the overall ratings, the sentiment labels, and the count of reviews.
* __Review Sentiment Count X Overall Review Rating (Figure 3):__ Figure 3 displays the frequency of ‘Positive’ or ‘Negative’ reviews stratified by the overall review ratings. Plotted in green are ‘Positive’ reviews while ‘Negative’ reviews are in red. From this plot, reviews given an overall rating of 3 or more typically have a comment with ‘Positive’ sentiment while reviews with ratings less than 3 typically have a negative sentiment.
* __Average Overall Rating X Average Student Sentiment per School (Figure 4):__ Figure 4 displays the results of aggregating both the sentiment of the review’s comments and the review’s overall rating to their respective averages for each school. These boxplots show schools with an average overall rating value less than 3 have an average student sentiment close to zero, while schools with a value greater than 3 have an average student sentiment close to 1. Schools with an average overall rating value of 3 have student sentiment values that vary more greatly across the domain.
* __Spaghetti Plot of the Two-Year Withdrawal Rate (Figure 5):__ Figure 5 displays the two-year withdrawal rate over time. On the horizontal axis, zero represents the earliest school year in the data (2001-2002) and each unit increase is the number of school years past the earliest school year. Each line is the trajectory of an individual school's withdrawal rate. The two-year withdrawal rate varies greatly depending on school but slightly decreases overall over the first ten years. After the 10-year mark (2011-2012), the withdrawal rate for many schools sharply increase, with some schools reaching above a 50% withdrawal rate. This suggests there may be some nonlinearity over time that would need a polynomial-based time parameter to account for.
* __Longitudinal Mean Description (Table 1):__ Table 1 displays the mean, average, and variance of the two-year withdrawal rate over time. Overall, the grand mean of withdrawal rate for Georgia schools is 23%. For comparison, the mean withdrawal rate for the most recent school year (2018-2019) was 9% higher at 32% while the value for the earliest school year was 7% lower at 17%. For the first eight to ten school years after 2001-2002, the mean two-year withdrawal rate oscillates between 15% and 20% but after the 2009-2010 school year the two-year withdrawal rate begins to increase to the 32% withdrawal rate observed for the most recent school year.
* __Likelihood Ratio Test:__ The Likelihood Ratio Test compares the Random Intercept model to an error-only model and is a test of whether the ICC is significantly greater than 0 (for p-values < 0.05). For this project the Empty Means, Random Intercept model produced an ICC of 0.2862 which suggests that 28.62% of the total variance is due to between-school differences. The Likelihood Ratio Test for this model produced a Chi-Square statistic of 692 and a p-value less than 0.0001, which suggests that the ICC value (0.2862) is significantly greater than zero. Since 71.38% of the total variance is due to within-school differences, moving forward with a longitudinal model is appropriate. 
* __Model Comparison of the Time-Only Models (Table 2):__ The worst model was the empty means, Random Intercept model which produced an AIC of -1,713 and a BIC of -1,708 (Table 2). Since the baseline ICC value displayed 72% of the total variation of this data is due to time-variant differences, the empty means, random intercept model was expected to be the worse because this model does not account for the change over time. Adding a fixed linear time slope to the model slightly improves the AIC (-1,981) and BIC (-1,976) while adding a random linear time slope greatly improves the AIC (-2,814) and BIC (-2,805). As discussed earlier, the spaghetti plot (Figure 5) displayed a nonlinear trend ten years after the earliest observed School Year. For this reason, a quadratic time parameter was added to the model to form a Random Quadratic Time Model which produced the best (lowest) AIC and BIC values (AIC: -2,981, BIC: -2964, Table 2). 
* __Final Model Comparison (Table 3):__ After selecting the best time-only model, the average student sentiment will be added to the model as a level-2 predictor. The Random Quadratic Time Model was observed to have the lowest AIC (-2,981) and BIC (-2,964) and was chosen as the best time-only model. After adding the predictor variable, average student sentiment, the AIC and BIC expectedly increased to -2,974 (AIC) and -2,957 (BIC). For both the time-only model and the predictor model, the p-value for the intercept and quadratic slope were both significant. The linear slope produced a relatively small p-value (0.04) in the time-only model but the p-value of the linear slope in predictor model was much larger (0.10). In addition, the predictor variable, average student sentiment, did not produce a small enough p-value to be significant to the model (0.3259). The main effect of student sentiment is the difference in the expected withdrawal rate at time zero. Since this estimate (-0.007) was negative, the change in withdrawal rate decreases for a unit increase in the average student sentiment. Similarly, the effect of withdrawal rate on linear time is the difference in the expected linear rate of change at time zero per school year. Since the estimate is negative (-0.003), the difference in the linear rate of change at time zero per school year is decreasing for a unital increase in the average student sentiment. Lastly, the effect of withdrawal rate on quadratic time is the difference in half the rate of acceleration/deceleration of linear rate of change per school year. The estimate (0.00099) found by this model is positive which suggests that the difference in half the rate of acceleration/deceleration of linear rate of change per school year is increasing with a unital increase in the average student sentiment. Since these values are very close to zero, the rate of change in withdrawal rate over time is very subtle. For The equation that describes the model for predicting the change in the two-year withdrawal rate by the average student sentiment over time is the following                                                   


## Folder structure

```
- readme.md
- scripts
---- readme.md
---- 
- data
---- readme.md
---- 
- deliverables
---- readme.md
---- Why.Withdraw.pptx
---- 
```

## Deliverables

* Why Withdraw.pptx

## Data sources

* CollegeScorecard: The CollegeScorecard dataset is released by the Department of Education through the Integrated Post-Secondary Education Data System (IPEDS) which collects nearly 3,000 parameters for 6,654 schools across the U.S per school year. In total, the CollegeScorecard dataset contains 176,688 observations across 25 school years (1996-1997 to 2020-2021). For this research project, four parameters were used:
    * INSTNM: The name of the school.
    * SCHOOL_YEAR: The school year (EX: 2020-2021). 
    * WDRAW_ORIG_YR2_RT: The withdrawal rate of students that withdrew from the original school they began at in two-years. This variable is a continuous value between 0 and 1.

* Rate My Professor: Since 1999 Rate My Professors has collected more than 15 million reviews for over 1.3 million professors each with up to 19 parameters. For this project, six parameters were used:
    * REVIEW_IDs: Three variables identify distinct schools (SID), professors (TID), and review instances (RID). 
    * DATE: The Month-Day-Year of the review.
    * OVERALL: A overall rating (1-5) given by the reviewer. 
    * TAGS: Buttonized TextStrings reviewers can leave in addition to the comments.
    * COMMENTS: The text reviews left by the student.

## Methods

The withdrawal rate for each of the 1,122 occasions across the 97 Georgia schools over 18 school years were collected for this longitudinal analysis. The dependent variable, withdrawal rate, is a continuous variable with a value between 0 and 1. Since the earliest timepoint observed was the 2001-2002 school year, this analysis will center the data around this school year. Prior to modelling the normality of the dependent variable was assessed using a residual scatterplot, Q-Q plot and Shapiro-Wilks test.

* __Text Preprocess:__ Prior to performing text analysis, the review comments must be made lowercase and cleaned of non-alphanumeric characters, html tags, URL links, and punctuation. In addition, the words within each review are removed if the word is considered a ‘stopword’ (‘a’, ‘do’, ‘not’), and put through a process of lemmatization to determine and replace the word with the canonical form. Removing stopwords resulted in the loss of several thousand reviews containing valuable data (such as ‘Take Them’ or ‘Do Not Take Them’). To mitigate this loss some stopwords, such as ‘not,’ ‘very,’ ‘do,’ and ‘did,’ were not removed. 
* __BERT:__ Bidirectional Encoder Representations from Transformers or BERT is a transformer-based machine learning technique used in natural language processing. A pretrained language model, such as ‘siebert/sentiment-roberta-large-English’, can be used in Hugging Face’s transformer pipeline for many natural language processing tasks, including sentiment analysis, zero-shot classification, topic modelling, and more. For this project, the sentiment analysis pipeline was used to return a label, either ‘Positive’ or ‘Negative,’ and a sentiment score ranging from 0 to 1.
* __Sentiment Analysis:__ Sentiment analysis is a natural language processing technique to systematically identify the positive or negative emotional polarity of a given text. It is also known as opinion mining and is typically used to gain insight into consumer reviews of a product. This project used a pretrained language model on the comments left by student reviewers on Rate My Professor and then aggregated the sentiment score, a value between 0 (Negative) and 1 (POSITIVE), for each school with the average. 
* __Descriptive:__ The mean, standard deviation, and variance withdrawal rates for each school and occasion were produced. A spaghetti plot visualized the mean pattern of withdrawal rate over time by school to assess the variance and covariance structure.
* __Longitudinal Analysis:__ Longitudinal data analysis seeks to partition the variance into time-invariant and time-variant pieces. This research project used a 2-level model where the level two unit of observation is schools, and the level one unit of observation is school year. Using the withdrawal rate as the dependent variable, time-only models were initially created and compared before including the average student sentiment. 
* __Test of Normally Distributed Residuals for the Dependent Variable:__ Prior to modelling, both the dependent variable and the residuals of a general model using the dependent variable were tested for normality. Although the numeric testing values displayed violations to the assumption of normality in the Shapiro-Wilks and the Kolmogorov-Smirnov tests, these tests are sensitive to sample size. After visually checking the Q-Q plot, it was determined that the dependent variable (Two-Year Withdrawal Rate) may be normally distributed.
* __Model Development and Fit:__ Each model created used the Restricted Maximum Likelihood (REML) estimation. In total, six different model structures was constructed for comparison using the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC). The first model created was an Unstructured, Saturated Means model but this model did not converge. The next model created was an Empty Means, Random Intercept Model which uses a Compound Symmetry covariance structure to partition the variance. This type of structure uses two parameters, the between-school variance and the residual within-school variance and allows for the retrieval of a baseline value for the Intraclass Correlation Value (ICC) in order to assess the proportion of Level 2, between-person variance. Next, fixed and random (both linear and quadratic) were added to the model in an iterative process, to determine if each effect contributed to the model fit by utilizing Wald test statistics and their corresponding  p-values for fixed effects and a likelihood ratio test using -2 log likelihood (-2LL) for the random effects. Model fit for both random and fixed effects was also assessed by utilizing Akaike Information Criteria (AIC) and Bayesian Information Criteria (BIC).

